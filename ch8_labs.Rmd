---
title: "8.3 Lab: Decision Trees"
author: "MSBA Team 5"
date: "02/15/2020"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
    theme: readable
    highlight: haddock
    fig_width: 16
    fig_height: 12
---
<style type="text/css">

body{ /* Normal  */
      font-size: 20px;
}

code.r{ /* Code block */
    font-size: 18px;
}
</style>

## 8.3.1 Fitting Classification Trees

```{r echo=TRUE,message=FALSE, warning=FALSE}
rm(list=ls())
# Load libraries function
installIfAbsentAndLoad <- function(neededVector) {
  for(thispackage in neededVector) {
    if( ! require(thispackage, character.only = T) )
    { install.packages(thispackage)}
    library(thispackage, character.only = T)
  }
}
par(mfrow =c(1,1))
needed <- c('tree', "ISLR")
library(ISLR)
# The tree library is used to construct classification and regression trees
library(tree)
```

### Dataset summary and processing

* We first use classification trees to analyze the Carseats data set

* Dataset: Sales of child car seats at 400 different stores

* Target variable: Sales which is unit sales (in thousands) at each location  

* ifelse() used to create a new binary variable called 'High'

* More details on the data set can be found here: https://rdrr.io/cran/ISLR/man/Carseats.html. 

```{r echo=TRUE,message=FALSE}
attach(Carseats)
head(Carseats)
High = ifelse(Sales <=8 , "No", "Yes")
Carseats = data.frame(Carseats, High)
```

### Fit a model

* Fit a classification tree to predict 'High' using all variables but 'Sales'

* A small deviance indicates a good fit to the training data 

```{r echo=TRUE}
# The syntax of the tree() function is quite similar to 
# that of the lm() function
tree.carseats = tree(High ~ . -Sales , Carseats)
# The summary() function lists the variables that are used as internal nodes,
# the number of terminal nodes and the training error rate.
summary(tree.carseats)
```

### Plot a model

* The most important indicator of Sales appears to be shelving location

* Typing in the name of the tree object gives us each branch of the tree

* Left to right: split criterion (e.g. Price <92.5), the number of observations in that branch, the deviance, the overall prediction for the branch (Yes or No), and the fraction of observations in that branch that take on values of Yes and No

* Branches that lead to terminal nodes are indicated using asterisks 

```{r echo=TRUE}
# Use the plot() function to display the tree strucutre
plot(tree.carseats)
# Use the text() function to display the node labels. 
text(tree.carseats,pretty=0)
# The argument pretty = 0 instructs R to include the category names 
# for any qualititaitve predictors, rather than simply displaying 
# a letter for each category.
tree.carseats
```

### Performance Evaluation (Classification)

* Estimate test error using cross validation  

* Split the data set and make predictions

* This approach leads to correct predictions for around 77% of the locations in the test data set

```{r echo=TRUE}
nrow(Carseats)
set.seed(2)
# take 200 rows at random from the Carseats dataset
train = sample(1:nrow(Carseats), 200)  
# the non-training rows are now the test set
Carseats.test = Carseats[-train,] 
# take the rows that were above that $8K threshold, which we called "High"
High.test = High[-train]  
# run our model using tree() package
tree.carseats = tree(High ~ . -Sales, Carseats, subset = train) 
# make prediction using test set
# type = "class" instructs R to return the actual class prediction
tree.pred = predict(tree.carseats, Carseats.test, type="class")  
table(tree.pred, High.test)
mytable <- table(tree.pred, High.test)
# from book error rate was 71.5%: (86+57)/200. We get 77%:
((mytable["No", "No"] + mytable["Yes","Yes"] )/ sum(mytable) )    
```

### Tree Pruning - Crossvalidation

* QUESTION: Will pruning the tree lead to improved results? 

* cv.tree() performs cross-validation to determine the optimal parameter

```{r echo=TRUE}

set.seed(3)
# FUN=prune.misclass is used in order to indicate that we want the
# classification error rate to guide the cross-validation process, 
# rather than the default for the cv.tree() function, which is deviance.
cv.carseats = cv.tree(tree.carseats, FUN=prune.misclass)
# The cv.tree() function reports size, the corresponding error rate (dev)
# and the value of the cost-complexity parameter used (k). 
names(cv.carseats)
cv.carseats
```

* Plot the error rate as a function of both size and k.

```{r echo=TRUE}
par(mfrow=c(1,1))
# Note that, despite the name, dev corresponds to the cross-validation 
# error rate in this instance. 
plot(cv.carseats$size, cv.carseats$dev, type="b")
plot(cv.carseats$k ,cv.carseats$dev, type="b")
# The tree with ? terminal nodes results in the lowest cross-validation 
# error rate, with ? cross validation errors. 
```

* prune.misclass() function can help prune the tree

* The pruning process produced a more interpreterable tree, but it has also improved the classification accuracy

```{r echo=TRUE}
prune.carseats = prune.misclass(tree.carseats, best=9)
plot(prune.carseats)
text(prune.carseats, pretty=0)
# How well does this pruned tree perform on the test data set?
tree.pred = predict(prune.carseats, Carseats.test, type="class")
table(tree.pred,High.test)
mytable_pruned <- table(tree.pred,High.test)
((mytable_pruned["No", "No"] + mytable_pruned["Yes","Yes"] )/ sum(mytable_pruned) ) 

# Now 77.5% of the test observatiosn are correctly classified
```

### Tree Pruning - Manually pick

* If we increase the value of 'best', we obtain a larger pruned tree

```{r echo=TRUE}
prune.carseats = prune.misclass(tree.carseats, best=15)# increased from 9
plot(prune.carseats)
text(prune.carseats, pretty=0)
tree.pred = predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
mytable_pruned15 <- table(tree.pred,High.test)
((mytable_pruned15["No", "No"] + mytable_pruned15["Yes","Yes"] )/ sum(mytable_pruned15) ) 
# The same accuracy
```

## 8.3.2 Fitting Regression Trees

### Dataset summary 

* Boston dataset comes from the library-Mass

* Target variable is medv which is the median value of homes in $1000's.

* More details can be found here: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html. 

```{r echo=TRUE}
rm(list=ls())
library(tree)
library(MASS)
head(Boston)
set.seed(1)
# Created a training set, and fit the tree to the training data.
train = sample(1:nrow(Boston), nrow(Boston)/2)

```

### Fit a model

* tree() is used again for fitting a tree model

* In the context of a regression tree, the deviance is the sum of squared errors for the tree

```{r echo=TRUE}
tree.boston = tree(medv~.,Boston,subset=train)
summary(tree.boston)
```

### Plot the tree

* The variable rm measures average number of rooms per dwelling

* Higher values of rm correspond to more expensive houses 

* The tree predicts median house price of $45,380 for larger homes in the suburbs with more rooms (rm >=7.553)

```{r echo=TRUE}
plot(tree.boston)
text(tree.boston,pretty=0)
```

### Tree pruning 

* cv.tree()  will be used to find the best parameter 

* plot() shows us the size of tree that gives the lowest sum of sqaured errors

* The unpruned tree gives us the lowest error
```{r echo=TRUE}
cv.boston=cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type='b')
```

* We can still prune if we want to by using prune.tree() function

```{r echo=TRUE}

prune.boston=prune.tree(tree.boston, best=5)
plot(prune.boston)
text(prune.boston, pretty = 0)
```

### Performance Evaluation (Regression)

* In keeping with cross-validation results, we use the unpruned tree to make predictions on the test set

* The Test MSE associated with the regression tree is 35.28688

* The square root of the MSE is around 5.9402, indicating that this model leads to test predictions that are within around $5,490 of the true median home value for the suburb

```{r echo=TRUE}
# predictions
yhat=predict(tree.boston, newdata=Boston[-train,])
# actual prices
boston.test=Boston[-train,"medv"]

plot(yhat,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
sqrt(mean((yhat-boston.test)^2))
```
